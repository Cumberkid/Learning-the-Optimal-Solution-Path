# -*- coding: utf-8 -*-
"""07 Learn Solution Path (LSP): Legendre.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Cumberkid/Learning-the-Optimal-Solution-Path/blob/main/experiments/fair-regression/notebooks/07%20Learn%20Solution%20Path%20(LSP)%3A%20Legendre.ipynb

# Import necessary libraries
"""

import numpy as np
import torch
from torch.utils.data import DataLoader  #for creating the dataset


device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using device: {device}")

import pandas as pd

# import importlib

"""## Import our own modules"""

# !rm -r Learning-the-Optimal-Solution-Path
!git clone https://github.com/Cumberkid/Learning-the-Optimal-Solution-Path.git

"""(Using Colab)"""

import sys

# Add the parent directory to sys.path
sys.path.append('/content/Learning-the-Optimal-Solution-Path')

import lib
# importlib.reload(lib)

from lib.utils_data import Regression_Data
from lib.lsp.basis_generator import phi_lam_legendre
from lib.lsp.basis_tf_module import Basis_TF_SGD
from lib.lsp.learn_solution_path import learn_solution_path

"""#Preliminaries
Recall that our method runs SGD over random $\tilde Î»$'s with a linear basis $\Phi(\tilde \lambda)$ of our choice. We want to approximate $\theta$ with $\Phi(\lambda)\beta$, so the objective function is $\min_\beta h(\Phi(\tilde\lambda)\beta, \tilde\lambda) = (1-\tilde\lambda) BCE(X_\text{pass}\Phi(\tilde\lambda)\beta,\ y_\text{pass}) + \tilde\lambda BCE(X_\text{fail}\Phi(\tilde\lambda)\beta,\ y_\text{fail})$. For each batch of training data set, we randomize $\tilde\lambda$. If batch size = 1, then this is equivalent to a standard SGD.

## Load data
"""

# file path for Colab. May need to change this
X_df = pd.read_csv('/content/Learning-the-Optimal-Solution-Path/experiments/fair-regression/data/X_processed.csv')
y_df = pd.read_csv('/content/Learning-the-Optimal-Solution-Path/experiments/fair-regression/data/y_processed.csv')

X = np.array(X_df)
y = np.array(y_df).squeeze()

full_data = Regression_Data(X, y)
# full gradient descent uses all data points
GD_data_loader = DataLoader(full_data, batch_size=len(full_data), shuffle=True, )
# stochastic gradient descent uses mini-batch
SGD_data_loader = DataLoader(full_data, batch_size=20, shuffle=True, )
# test data
test_data_loader = DataLoader(full_data, batch_size=len(full_data), shuffle=False, )

lam_max = 1
lam_min = 0
input_dim = X.shape[1]
criterion=torch.nn.BCELoss()

"""## Choose basis functions

We use Legendre polynomials with degree $\leq n$ as the basis vectors for $\Phi(\lambda)$.
"""

phi_lam = phi_lam_legendre

criterion = torch.nn.BCELoss()
input_dim = X.shape[1]

"""# Exact Gradient Oracle Constant LR"""

from google.colab import drive
drive.mount('/content/drive')

import time

# Read the CSV file into a DataFrame
truth = pd.read_csv('/content/Learning-the-Optimal-Solution-Path/experiments/fair-regression/notebooks/results/exact_soln_list.csv')

# Display the DataFrame
truth

selected_columns = ['theta_0', 'theta_1', 'theta_2', 'theta_3', 'theta_4',
                    'theta_5', 'theta_6', 'theta_7', 'theta_8', 'theta_9',
                    'theta_10', 'theta_11', 'theta_12', 'theta_13', 'theta_14',
                    'theta_15', 'theta_16', 'theta_17', 'theta_18', 'theta_19',
                    'theta_20', 'theta_21', 'theta_22', 'theta_23', 'theta_24',
                    'theta_25', 'theta_26', 'theta_27', 'theta_28', 'theta_29',
                    'theta_30', 'theta_31', 'theta_32', 'theta_33', 'theta_34',
                    'theta_35', 'theta_36', 'theta_37', 'theta_38', 'theta_39',
                    'theta_40', 'theta_41', 'theta_42', 'theta_43', 'theta_44',
                    'theta_45']
true_thetas = truth[selected_columns].to_numpy()
true_losses = truth['losses'].to_numpy()

"""We use diminishing learning rate for better demonstrate convergence. If we use a constant learning rate, the solution path error will eventually do a random walk after descending to a certain threshold value.

We will see this random walk in a plot later.
"""

epochs = 20000

"""## num basis func = 3"""

basis_dim = 3
lr = 0.5

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                          GD_data_loader, test_data_loader, criterion,
                                                                          lam_min, lam_max, true_losses,
                                                                          lr=lr, diminish=True, gamma=0.97, dim_step=100,
                                                                          obj='fairness',
                                                                          record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

file_path = 'SGD_results_exact_diminish.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

df['sup_err_3'] = sup_err_history

# Save the DataFrame to a CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 5"""

basis_dim = 5
lr = 0.5

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                          GD_data_loader, test_data_loader, criterion,
                                                                          lam_min, lam_max, true_losses,
                                                                          lr=lr, diminish=True, gamma=0.97, dim_step=100,
                                                                          obj='fairness',
                                                                          record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

# file_path = '/content/drive/MyDrive/Experiments/SGD_results_exact.csv'
file_path = 'SGD_results_exact_diminish.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

df['sup_err_5'] = sup_err_history

# Save the DataFrame to a CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 7"""

basis_dim = 7
lr = 0.5

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                          GD_data_loader, test_data_loader, criterion,
                                                                          lam_min, lam_max, true_losses,
                                                                          lr=lr, diminish=True, gamma=0.97, dim_step=100,
                                                                          obj='fairness',
                                                                          record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

file_path = 'SGD_results_exact_diminish.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

df['sup_err_7'] = sup_err_history

# Save the DataFrame to a CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 9"""

basis_dim = 9
lr = 0.4

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                          GD_data_loader, test_data_loader, criterion,
                                                                          lam_min, lam_max, true_losses,
                                                                          lr=lr, diminish=False, gamma=0.99, dim_step=100,
                                                                          obj='fairness',
                                                                          record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

# file_path = '/content/drive/MyDrive/Experiments/SGD_results_exact.csv'
file_path = 'SGD_results_exact_diminish.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

df['sup_err_9'] = sup_err_history

# Save the DataFrame to a CSV file
df.to_csv(file_path, index=False)

"""# Exact Gradient Oracle Diminishing LR

Train the SGD model for our method using exact gradient and record the sup error along the solution path ($\epsilon$) achieved after executing some number of gradient calls (epochs).

We use the previously tuned learning rate $0.5$.
"""

epochs = 200000

"""## num basis func = 3"""

basis_dim = 3
lr = 0.5

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   GD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   lr=lr, SGD=False, obj='fairness',
                                                                   intercept=True, trace_frequency=1000)
sup_err_history = np.array(sup_err_history)

# import csv

# csv_file_path = '/content/drive/MyDrive/Experiments/SGD_results_exact.csv'

# # Open the CSV file for writing
# with open(csv_file_path, 'w', newline='') as csv_file:
#     # Create a CSV writer
#     csv_writer = csv.writer(csv_file)

#     # Write the headers
#     csv_writer.writerow(['num_itr', 'sup_err_13'])

#     # Write the data from your lists
#     for i in range(len(epochs)):
#         csv_writer.writerow([epochs[i], sup_err_history[i]])

# for i in range(len(sup_err_history)):
#     sup_err_history[i] = np.min(sup_err_history[:i+1])

# file_path = '/content/drive/MyDrive/Experiments/SGD_results_exact.csv'
file_path = 'SGD_results_exact.csv'

SGD_results_exact = pd.DataFrame(np.column_stack((num_itr_history, sup_err_history)), columns=['num_itr', 'sup_err_3'])

# Save the DataFrame to a CSV file
SGD_results_exact.to_csv(file_path, index=False)

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Display the DataFrame
df

"""## num basis func = 5"""

basis_dim = 5
lr = 0.5

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   GD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   lr=lr, SGD=False, obj='fairness',
                                                                   intercept=True, trace_frequency=1000)
sup_err_history = np.array(sup_err_history)

# Add a new column for a different basis dimension
df['sup_err_5'] = sup_err_history

# Save the updated DataFrame back to the CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 7"""

epochs = 530000

basis_dim = 7
lr = 0.5

num_itr_history, sup_err_history = SGD.learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   GD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   lr=lr, SGD=False, obj='fairness',
                                                                   intercept=True, trace_frequency=1000)
sup_err_history = np.array(sup_err_history)

# Add a new column for a different basis dimension
df['sup_err_7'] = sup_err_history[:200000]

# Save the updated DataFrame back to the CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 9"""

basis_dim = 9
lr = 0.25

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   GD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   lr=lr, SGD=False, obj='fairness',
                                                                   intercept=True, trace_frequency=1000)
sup_err_history = np.array(sup_err_history)

# file_path = '/content/drive/MyDrive/Experiments/SGD_results_exact.csv'
df = pd.read_csv(file_path)

# df = df.rename(columns={'sup_err': 'sup_err_9'})

# Add a new column for a different basis dimension
df['sup_err_9'] = sup_err_history

# Save the updated DataFrame back to the CSV file
df.to_csv(file_path, index=False)

"""# Noisy Gradient Oracle Diminishing LR

Train the SGD model for our method using noisy gradient (mini-batch SGD) and record the sup error along the solution path ($\epsilon$) achieved after executing some number of gradient calls (epochs).

We use the previously tuned shrinking rate $\alpha = 4$.
"""

epochs = 20000

"""## Num basis func = 3"""

basis_dim = 3
alpha = 4
init_lr = 0.1

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                          SGD_data_loader, test_data_loader, criterion,
                                                                          lam_min, lam_max, true_losses,
                                                                          alpha=alpha, init_lr=init_lr, SGD=True,
                                                                          obj='fairness',
                                                                          record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

# import csv

# csv_file_path = '/content/drive/MyDrive/Experiments/SGD_results_noisy.csv'

# # Open the CSV file for writing
# with open(csv_file_path, 'w', newline='') as csv_file:
#     # Create a CSV writer
#     csv_writer = csv.writer(csv_file)

#     # Write the headers
#     csv_writer.writerow(['num_itr', 'sup_err_3'])

#     # Write the data from your lists
#     for i in range(len(epochs)):
#         csv_writer.writerow([epochs[i], sup_err_history[i]])

# for i in range(len(sup_err_history)):
#     sup_err_history[i] = np.min(sup_err_history[:i+1])
# # print(model_SGD.linear.weight)

file_path = 'SGD_results_noisy.csv'

SGD_results_noisy = pd.DataFrame(np.column_stack((num_itr_history, sup_err_history)), columns=['num_itr', 'sup_err_3'])

# Save the DataFrame to a CSV file
SGD_results_noisy.to_csv(file_path, index=False)

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Display the DataFrame
df

"""## Num basis func = 4"""

basis_dim = 4
alpha = 4
init_lr = 0.1

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   SGD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   alpha=alpha, init_lr=init_lr, SGD=True,
                                                                   obj='fairness',
                                                                   record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

file_path = 'SGD_results_noisy.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Add a new column for basis dimension 5
df['sup_err_4'] = sup_err_history

# Save the updated DataFrame back to the CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 5"""

basis_dim = 5
alpha = 4
init_lr = 0.1

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   SGD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   alpha=alpha, init_lr=init_lr, SGD=True,
                                                                   obj='fairness',
                                                                   record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

file_path = 'SGD_results_noisy.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Add a new column for basis dimension 5
df['sup_err_5'] = sup_err_history

# Save the updated DataFrame back to the CSV file
df.to_csv(file_path, index=False)

"""## num basis func = 7"""

basis_dim = 7
alpha = 4
init_lr = 0.1

np.random.seed(8675309)
torch.manual_seed(8675309)

num_itr_history, sup_err_history, model = learn_solution_path(input_dim, basis_dim, phi_lam, epochs,
                                                                   SGD_data_loader, test_data_loader, criterion,
                                                                   lam_min, lam_max, true_losses,
                                                                   alpha=alpha, init_lr=init_lr, SGD=True,
                                                                   obj='fairness',
                                                                   record_frequency=50, trace_frequency=100)
sup_err_history = np.array(sup_err_history)

# file_path = '/content/drive/MyDrive/Experiments/SGD_results_noisy.csv'
df = pd.read_csv(file_path)

# Add a new column for a different basis dimension
df['sup_err_7'] = sup_err_history

# Save the updated DataFrame back to the CSV file
df.to_csv(file_path, index=False)

# # plot result of learning the solution path
# plt.figure(figsize=(18,7))
# plt.plot(torch.log(torch.tensor(sup_err_history)), epochs, '--o')
# plt.ylabel('# of Gradient Calls')
# plt.xlabel('$\\log\\epsilon$')
# plt.legend()
# plt.grid(True)