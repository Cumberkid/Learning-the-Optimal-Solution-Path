# -*- coding: utf-8 -*-
"""Copy of 06 Naive Grid Search (NGS).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SyskMZ22wZtVFj-bLQzVYtUzqiV_SwK9

# Import necessary libraries
"""

import numpy as np
import torch
from torch.utils.data import DataLoader  #for creating the dataset


device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using device: {device}")

import pandas as pd

import matplotlib.pyplot as plt
import matplotlib as mpl

# import importlib

"""## Import our own modules"""

# !rm -r Learning-the-Optimal-Solution-Path
!git clone https://github.com/Cumberkid/Learning-the-Optimal-Solution-Path.git

"""(Using Colab)"""

import sys

# Add the parent directory to sys.path
sys.path.append('/content/Learning-the-Optimal-Solution-Path')

import lib
# importlib.reload(lib)

from lib.utils_data import Regression_Data
from lib.ngs.naive_grid_search import naive_grid_search
from lib.ngs.utils_ngs import get_sup_error

"""# Load data"""

# file path for Colab. May need to change this
X_df = pd.read_csv('/content/Learning-the-Optimal-Solution-Path/experiments/fair-regression/data/X_processed.csv')
y_df = pd.read_csv('/content/Learning-the-Optimal-Solution-Path/experiments/fair-regression/data/y_processed.csv')

X = np.array(X_df)
y = np.array(y_df).squeeze()

full_data = Regression_Data(X, y)
# full gradient descent uses all data points
GD_data_loader = DataLoader(full_data, batch_size=len(full_data), shuffle=True, )
# stochastic gradient descent uses mini-batch
SGD_data_loader = DataLoader(full_data, batch_size=20, shuffle=True, )
# test data
test_data_loader = DataLoader(full_data, batch_size=len(full_data), shuffle=False, )

lam_max = 1
lam_min = 0
input_dim = X.shape[1]
criterion=torch.nn.BCELoss()

"""# Naive Grid Search"""

from google.colab import drive
drive.mount('/content/drive')

import time

# Read the CSV file into a DataFrame
truth = pd.read_csv('/content/Learning-the-Optimal-Solution-Path/experiments/fair-regression/notebooks/results/exact_soln_list.csv')

# Display the DataFrame
truth

selected_columns = ['theta_0', 'theta_1', 'theta_2', 'theta_3', 'theta_4',
                    'theta_5', 'theta_6', 'theta_7', 'theta_8', 'theta_9',
                    'theta_10', 'theta_11', 'theta_12', 'theta_13', 'theta_14',
                    'theta_15', 'theta_16', 'theta_17', 'theta_18', 'theta_19',
                    'theta_20', 'theta_21', 'theta_22', 'theta_23', 'theta_24',
                    'theta_25', 'theta_26', 'theta_27', 'theta_28', 'theta_29',
                    'theta_30', 'theta_31', 'theta_32', 'theta_33', 'theta_34',
                    'theta_35', 'theta_36', 'theta_37', 'theta_38', 'theta_39',
                    'theta_40', 'theta_41', 'theta_42', 'theta_43', 'theta_44',
                    'theta_45']
true_thetas = truth[selected_columns].to_numpy()
true_losses = truth['losses'].to_numpy()

"""# Exact Gradient Oracle Constant LR

Use the previously tuned lr = 2.
"""

lam_max = 1
lam_min = 0
lr = 2
max_epochs = 5000
# a list of solution accuracy delta wish to be achieved
delta_list = 0.5 ** np.arange(3, 9, 0.2)

total_itr_list = []
sup_error_list = []
for delta in delta_list:
    # number of grids according to 1/sqrt(delta)
    num_grid = round(10 / np.sqrt(delta))

    start_time = time.time()
    total_itr, reg_params, intercepts, weights = naive_grid_search(lam_min=lam_min, lam_max=lam_max,
                                num_grid=num_grid, epochs=max_epochs, loss_fn=criterion,
                                trainDataLoader=GD_data_loader,
                                data_input_dim=input_dim, obj='fairness',
                                lr=lr, SGD=False, testDataLoader=test_data_loader,
                                true_loss_list=true_losses, stopping_criterion=delta)

    end_time = time.time()
    execution_time = end_time - start_time

    total_itr_list.append(total_itr)

    sup_error = get_sup_error(lam_min, lam_max, true_losses, intercepts,
                                  weights, reg_params, test_data_loader, criterion, obj='fairness')

    sup_error_list.append(sup_error)

    print(f"grid #: {num_grid}\t total iteration #: {total_itr}\t sup error: {sup_error}\t Execution time: {execution_time} seconds")

total_itr_list = np.array(total_itr_list)
sup_error_list = np.array(sup_error_list)

df = pd.DataFrame(np.column_stack((total_itr_list, sup_error_list)), columns=['num_itr', 'sup_err'])

# Save the DataFrame to a CSV file
df.to_csv('NGS_results_exact.csv', index=False)

# Read the CSV file into a DataFrame
df = pd.read_csv('NGS_results_exact.csv')

# import csv

# csv_file_path = '/content/drive/MyDrive/Experiments/NGS_results_exact.csv'

# # Open the CSV file for writing
# with open(csv_file_path, 'w', newline='') as csv_file:
#     # Create a CSV writer
#     csv_writer = csv.writer(csv_file)

#     # Write the headers
#     csv_writer.writerow(['num_itr', 'sup_err'])

#     # Write the data from your lists
#     for i in range(len(total_itr_list)):
#         csv_writer.writerow([total_itr_list[i], sup_error_list[i]])

# for i in range(len(sup_error_list)):
#     sup_error_list[i] = np.min(sup_error_list[:i+1])

# plot result of learning the solution path
plt.figure(figsize=(10,10))
plt.plot(total_itr_list, np.log(sup_error_list), '--o')
plt.xlabel('# of Gradient Calls')
plt.ylabel('$\\log\\epsilon$')
plt.legend()
plt.grid(True)

"""# Noisy Gradient Oracle Diminishing LR

Use the previously tuned diminishing factor $\alpha = 8$ where lr = $\alpha/T$.
"""

lam_max = 1
lam_min = 0
alpha = 2**3
max_epochs = 5000
# a list of solution accuracy delta wish to be achieved
delta_list = 0.5 ** np.arange(3, 7, 0.125)

total_itr_list = []
sup_error_list = []
np.random.seed(8675309)
torch.manual_seed(8675309)
for delta in delta_list:
    # number of grids according to 1/sqrt(delta)
    num_grid = round(10 / np.sqrt(delta))

    start_time = time.time()
    total_itr, reg_params, intercepts, weights = naive_grid_search(lam_min=lam_min, lam_max=lam_max,
                                num_grid=num_grid, epochs=max_epochs, loss_fn=criterion,
                                trainDataLoader=SGD_data_loader, data_input_dim=input_dim, obj='fairness',
                                alpha=alpha, init_lr = 1, SGD=True, testDataLoader=test_data_loader,
                                true_loss_list=true_losses, stopping_criterion=delta)

    end_time = time.time()
    execution_time = end_time - start_time

    total_itr_list.append(total_itr)

    sup_error = get_sup_error(lam_min, lam_max, true_losses, intercepts,
                                  weights, reg_params, test_data_loader, criterion, obj='fairness')

    sup_error_list.append(sup_error)

    print(f"grid #: {num_grid}\t total iteration #: {total_itr}\t sup error: {sup_error}\t Execution time: {execution_time} seconds")

total_itr_list = np.array(total_itr_list)
sup_error_list = np.array(sup_error_list)

df = pd.DataFrame(np.column_stack((total_itr_list, sup_error_list)), columns=['num_itr', 'sup_err'])

# Save the DataFrame to a CSV file
df.to_csv('NGS_results_noisy.csv', index=False)

# Read the CSV file into a DataFrame
df = pd.read_csv('NGS_results_noisy.csv')

# # plot result of learning the solution path
# plt.figure(figsize=(10,10))
# plt.plot(total_itr_list, np.log(sup_error_list), '--o', )
# plt.ylabel('# of Gradient Calls')
# plt.xlabel('$\\log\\epsilon$')
# plt.legend()
# plt.grid(True)