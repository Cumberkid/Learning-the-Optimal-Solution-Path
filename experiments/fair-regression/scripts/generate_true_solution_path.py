# -*- coding: utf-8 -*-
"""Copy of 05 Generate True Solution Path.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YB3XI6XJ5h895uBSn22-EFhSl88Puw8g

# Import necessary libraries
"""

import numpy as np
import torch
from torch.utils.data import DataLoader  #for creating the dataset


device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

import pandas as pd

"""## Import our own modules"""

import lib

from lib.utils_data import Regression_Data
from lib.ngs.naive_grid_search import naive_grid_search
from lib.ngs.utils_ngs import get_losses

"""# The True Solution Path

We use the Naive Grid Search with $2^{10}$ grids, trained by full gradient descent with tuned learning rate = $2$ and 5000 iterations on each grid, to generate a true solution path.
"""

# file path for Colab. May need to change this
X_df = pd.read_csv('../data/X_processed.csv')
y_df = pd.read_csv('../data/y_processed.csv')

X = np.array(X_df)
y = np.array(y_df).squeeze()

full_data = Regression_Data(X, y)
# full gradient descent uses all data points
GD_data_loader = DataLoader(full_data, batch_size=len(full_data), shuffle=True, )
# stochastic gradient descent uses mini-batch
SGD_data_loader = DataLoader(full_data, batch_size=5, shuffle=True, )
# test data
test_data_loader = DataLoader(full_data, batch_size=len(full_data), shuffle=False, )

lam_max = 1
lam_min = 0
input_dim = X.shape[1]
criterion=torch.nn.BCELoss()

num_grid = 2**10
lambdas = np.linspace(lam_max, lam_min, num_grid)
fine_delta_lam = (lam_max - lam_min)/(num_grid - 1)

epochs = 5000
lr = 2

total_itr, reg_params, intercepts, weights = naive_grid_search(lam_min=lam_min, lam_max=lam_max,
                                num_grid=num_grid, epochs=epochs, loss_fn=criterion,
                                trainDataLoader=GD_data_loader,
                                data_input_dim=input_dim, obj='fairness',
                                lr=lr, SGD=False)

losses = get_losses(lam_min, lam_max, fine_delta_lam, intercepts,
                              weights, reg_params, test_data_loader, criterion, obj='fairness')

thetas = np.array(weights)
print(thetas.shape)

headers = ['losses', 'theta_0', 'theta_1', 'theta_2', 'theta_3',
            'theta_4', 'theta_5', 'theta_6', 'theta_7', 'theta_8',
            'theta_9', 'theta_10', 'theta_11', 'theta_12', 'theta_13',
            'theta_14', 'theta_15', 'theta_16', 'theta_17', 'theta_18',
            'theta_19', 'theta_20', 'theta_21', 'theta_22', 'theta_23',
            'theta_24', 'theta_25', 'theta_26', 'theta_27', 'theta_28',
            'theta_29', 'theta_30', 'theta_31', 'theta_32', 'theta_33',
            'theta_34', 'theta_35', 'theta_36', 'theta_37', 'theta_38',
            'theta_39', 'theta_40', 'theta_41', 'theta_42', 'theta_43',
            'theta_44', 'theta_45']

exact_soln_list = pd.DataFrame(np.column_stack((losses, intercepts, thetas)), columns=headers)

# Save the DataFrame to a CSV file
exact_soln_list.to_csv('exact_soln_list.csv', index=False)
