# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14tdaulQMMrW53XVmxCkNtwUwvRZ23W0E

# Logistic regression through the SGD interface of NN
"""
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader  #for creating the dataset

device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

#Prep data for Pytorch Dataloader
class Regression_Data(Dataset):
    def __init__(self, X, y):
        self.X = X  if isinstance(X, torch.Tensor) else torch.FloatTensor(X)
        self.y = y  if isinstance(y, torch.Tensor) else torch.FloatTensor(y)
        self.input_dim = self.X.shape[1]

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        return self.X[idx,:], self.y[idx]

    def get_y(self):
        return self.y

# this initializes with random weights. Need to either set a seed or force initialization somewhere for reproducibility.
# automatically fits an intercept. To turn off intercept, set bias=False in nn.Linear()
class Logistic_Regression(nn.Module):
    def __init__(self, input_dim, output_dim, reg_param, init_weight):
        super(Logistic_Regression, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim, bias=True)
        self.actv = nn.Sigmoid()
        self.reg_param = reg_param

        # initialize for better performance
        with torch.no_grad():
          self.linear.weight.copy_(init_weight)
          self.linear.bias.data.fill_(0)

    def forward(self, x):
        return self.actv(self.linear(x))
        
    def ridge_term(self):
        return self.linear.weight.norm(p=2)**2 + self.linear.bias.norm(p=2)**2

"""The "train" function executes optimization on the input dataset w.r.t. the input loss function with the input optimizer. We will use the pytorch built-in SGD optimizer later, but note that this optimizer is actually just a deterministic gradient descent program.

To randomize for SGD, we notice that the loss function is a sum of losses of all training data points, and a standard SGD would randomly choose one of those points to descend on at each step of descent.

To speed up, we use a batch of data points to replace a single data point at each step of descent. When batch size = 1, this is equivalent to a standard SGD; and when batch size = training set size, this is simply a deterministic gradient descent.
"""

# trace_frequency is measured in number of batches. -1 means don't print
def train(dataloader, model, loss_fn, optimizer, trace_frequency = -1):
    # size = len(dataloader.dataset)
    model.train()
    # here, the "batch" notion takes care of randomization
    for batch, (X_train, y_train) in enumerate(dataloader):
        X_train, y_train = X_train.to(device), y_train.to(device)
        # print(batch, len(X_train))

        # Compute predicted y_hat
        pred = model(X_train)
        # With regularization
        loss = (1 - model.reg_param) * loss_fn(pred.view(-1, 1), y_train.view(-1, 1))
        loss += model.reg_param * 0.5 * model.ridge_term()

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # if (trace_frequency > 0) & (batch % trace_frequency == 0):
        #     loss, current = loss.item(), (batch + 1) * len(X_train)
        #     print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

"""The "test" function defined here is our objective function $h(\theta, \lambda) = (1-\lambda)BCE(X\theta, y) + \frac{\lambda}{2}\|\theta\|^2$. The linear weight from the above trained model is our $\theta$."""

# Test function
def test(dataloader, model, loss_fn, lam):
    model.eval() # important
    with torch.no_grad():  # makes sure we don't corrupt gradients and is faster
        for batch, (X_test, y_test) in enumerate(dataloader):
            X_test, y_test = X_test.to(device), y_test.to(device)

            # Compute prediction error
            pred = model(X_test)

            oos = (1 - lam) * loss_fn(pred.view(-1, 1), y_test.view(-1, 1))
            oos += lam * 0.5 * model.ridge_term()

    return oos.item()
    


#A silly synthetic data example
def gen_data(n_obs_1, n_obs_2, n_var):
    # Draw X randomly
    data_1 = np.random.multivariate_normal(
             np.zeros(n_var),
             np.eye(n_var),
             n_obs_1
             )

    data_2 = np.random.multivariate_normal(
             np.ones(n_var),
             np.eye(n_var),
             n_obs_2
             )

    data = np.vstack((data_1, data_2))

    # Create corresponding labels (0 for the first distribution, 1 for the second)
    labels = np.hstack((np.zeros(n_obs_1), np.ones(n_obs_2)))

    # Shuffle the data and labels
    shuffled_indices = np.random.permutation(data.shape[0])
    data = data[shuffled_indices]
    labels = labels[shuffled_indices]
    return data.astype(np.float32), labels.astype(np.float32)

"""# Helper functions to test convergence"""

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def gradient_descent(X, y, weights, lam, learning_rate, num_iterations):
    m = len(y)
    for i in range(num_iterations):
        y_pred = sigmoid(np.dot(X, weights))
        gradient = (1-lam) * np.dot(X.T, y_pred - y) / m + lam * weights
        weights -= learning_rate * gradient
    return weights

def logit_by_hand(weight, intercept, lam, X_set, y_set):
    # weight = torch.zeros(input_dim, 1)
    pred = torch.sigmoid(torch.mm(X_set, weight) + intercept)
    # print(pred)
    criterion = torch.nn.BCELoss()
    soln = (1 - lam) * criterion(torch.squeeze(pred), y_set)
    soln += lam * 0.5 * (torch.squeeze(weight).norm(p=2)**2 + torch.squeeze(intercept)**2)

    return soln, pred